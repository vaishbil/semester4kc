prac 1:Load a Dataset, Identify Missing Values, Handle Them, and Remove Outliers
#pip install pandas 
#pip install seaborn
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load inbuilt dataset (Titanic)
df = sns.load_dataset("titanic")

# Identify missing values
print("Missing Values:\n", df.isnull().sum())

# Handle missing values:
# - Fill numerical columns with the median
# - Fill categorical columns with the mode
for col in df.select_dtypes(include=['number']).columns:
    df[col].fillna(df[col].median(), inplace=True)

for col in df.select_dtypes(include=['object']).columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Detect and remove outliers using IQR method
Q1 = df.select_dtypes(include=['number']).quantile(0.25)
Q3 = df.select_dtypes(include=['number']).quantile(0.75)
IQR = Q3 - Q1

df_cleaned = df[~((df.select_dtypes(include=['number']) < (Q1 - 1.5 * IQR)) |
                  (df.select_dtypes(include=['number']) > (Q3 + 1.5 * IQR))).any(axis=1)]

print("\nShape before outlier removal:", df.shape)
print("Shape after outlier removal:", df_cleaned.shape)


prac 2: Practical to Implement: Perform feature scaling (standardization, normalization) and 
transformation on a numerical dataset. 
#pip install scikit-learn

from sklearn.datasets import load_diabetes
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer
import pandas as pd
import numpy as np

# Load inbuilt dataset (Diabetes dataset)
data = load_diabetes()
df = pd.DataFrame(data.data, columns=data.feature_names)

# Standardization (Z-score)
scaler_standard = StandardScaler()
df_standardized = pd.DataFrame(scaler_standard.fit_transform(df), columns=df.columns)

# Normalization (Min-Max Scaling)
scaler_minmax = MinMaxScaler()
df_normalized = pd.DataFrame(scaler_minmax.fit_transform(df), columns=df.columns)

# Log Transformation (for skewed data)
df_transformed = df.copy()
df_transformed = df_transformed.apply(lambda x: np.log1p(x))

print("\nOriginal Data (first 5 rows):\n", df.head())
print("\nStandardized Data:\n", df_standardized.head())
print("\nNormalized Data:\n", df_normalized.head())
print("\nLog Transformed Data:\n", df_transformed.head())



Prac 3: Handle structured (CSV), semi-structured (JSON), and unstructured data (text files) to 
understand differences. 
import json
import pandas as pd
import seaborn as sns
import nltk
from nltk.corpus import gutenberg

     

# Handling Structured Data (CSV) - Using Seaborn's Titanic dataset
df_csv = sns.load_dataset("titanic")  # Load inbuilt Titanic dataset
print("\nStructured Data (CSV):\n", df_csv.head())

# Handling Semi-Structured Data (JSON) - Using Sample JSON Dictionary
data_json = {
    "name": "John Doe",
    "age": 30,
    "city": "New York",
    "skills": ["Python", "Data Science", "Machine Learning"]
}
df_json = pd.json_normalize(data_json)  # Convert JSON to DataFrame
print("\nSemi-Structured Data (JSON):\n", df_json)

# Handling Unstructured Data (Text) - Using NLTK Gutenberg Corpus
nltk.download('gutenberg')
text_data = gutenberg.raw('shakespeare-hamlet.txt')  # Load Hamlet text
print("\nUnstructured Data (Text File):\n", text_data[:200])  # Show first 200 characters

     

prac4:Compute measures of central tendency (mean, median, mode) and 
dispersion (variance, standard deviation).
import pandas as pd
import seaborn as sns
import numpy as np
from scipy import stats
# Load inbuilt dataset (Titanic)
df = sns.load_dataset("titanic")
# Select numerical columns only
df_numeric = df.select_dtypes(include=['number'])

# Compute Measures of Central Tendency
mean_values = df_numeric.mean()
median_values = df_numeric.median()
mode_values = df_numeric.mode().iloc[0]  # Mode might return multiple values, take first row

# Compute Measures of Dispersion
variance_values = df_numeric.var()
std_dev_values = df_numeric.std()

# Display results
print("Mean Values:\n", mean_values)
print("\nMedian Values:\n", median_values)
print("\nMode Values:\n", mode_values)
print("\nVariance Values:\n", variance_values)
print("\nStandard Deviation Values:\n", std_dev_values)



Prac 5 : Analyze data distribution by plotting histograms and calculating skewness and kurtosis.

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Load inbuilt dataset (Penguins)
df = sns.load_dataset("penguins")
# Select numerical columns only
df_numeric = df.select_dtypes(include=['number']).dropna()  # Drop NaN values for calculations

# Plot histograms
df_numeric.hist(bins=20, figsize=(10, 6), edgecolor='black', grid=False)
plt.suptitle("Histograms of Numerical Features in Penguins Dataset", fontsize=14)
plt.show()

# Compute Skewness
skewness_values = df_numeric.skew()
print("\nSkewness Values:\n", skewness_values)

# Compute Kurtosis
kurtosis_values = df_numeric.kurtosis()
print("\nKurtosis Values:\n", kurtosis_values)


prac 6 : Create scatter plots, heatmaps, and boxplots to visualize relationships and outliers in the 
data.

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load inbuilt dataset (Penguins)
df = sns.load_dataset("penguins").dropna()  # Drop NaN values for visualization

# Scatter Plot: Relationship between Flipper Length and Body Mass
plt.figure(figsize=(8, 5))
sns.scatterplot(data=df, x="flipper_length_mm", y="body_mass_g", hue="species", style="sex")
plt.title("Scatter Plot: Flipper Length vs. Body Mass")
plt.show()

# Heatmap: Correlation between numerical features
plt.figure(figsize=(6, 4))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Heatmap of Numerical Features in Penguins Dataset")
plt.show()

# Boxplot: Detect outliers in Bill Length by Species
plt.figure(figsize=(8, 5))
sns.boxplot(data=df, x="species", y="bill_length_mm", hue="sex")
plt.title("Boxplot: Bill Length by Species")
plt.show()


prac 7: K-NN and Decision Tree Classifier 
#pip install scikit-learn 
from sklearn.datasets import load_wine 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import classification_report 
# Load dataset 
data = load_wine() 
X, y = data.data, data.target 
# Train-test split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) 
# Scale data 
scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test) 
# k-NN 
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train) 
print("k-NN Classification Report:") 
print(classification_report(y_test, knn.predict(X_test))) 
# Decision Tree 
tree = DecisionTreeClassifier() 
tree.fit(X_train, y_train) 
print("Decision Tree Classification Report:") 
print(classification_report(y_test, tree.predict(X_test)))


prac 8: Simple and Multiple Linear Regression
 #pip install statsmodels
import seaborn as sns 
import statsmodels.api as sm 
# Load dataset 
tips = sns.load_dataset('tips') 
# Simple Linear Regression (Tip ~ Total Bill) 
X = tips[['total_bill']] 
y = tips['tip'] 
X = sm.add_constant(X) 
model_simple = sm.OLS(y, X).fit() 
print(model_simple.summary()) 
# Multiple Linear Regression (Tip ~ Total Bill + Size) 
X_multi = tips[['total_bill', 'size']] 
X_multi = sm.add_constant(X_multi) 
model_multi = sm.OLS(y, X_multi).fit() 
print(model_multi.summary())


prac 9 : K-means and DBSCAN Clustering 
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Load dataset
data = load_wine()
X = StandardScaler().fit_transform(data.data)
# Reduce dimensions for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
# K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
labels_kmeans = kmeans.fit_predict(X)
# DBSCAN clustering
dbscan = DBSCAN(eps=1.5, min_samples=5)
labels_dbscan = dbscan.fit_predict(X)
# Plot K-means
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_kmeans, cmap='Set1')
plt.title('K-Means Clustering')
plt.show()
# Plot DBSCAN
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_dbscan, cmap='Set2')
plt.title('DBSCAN Clustering')
plt.show()


prac 10: Apriori Algorithm for Market Basket Analysis 
import pandas as pd
import seaborn as sns
from itertools import combinations
from collections import defaultdict

# Load inbuilt dataset
df = sns.load_dataset('titanic')
# We'll treat 'passenger class', 'sex', and 'embarked' as items "in a basket" (like items bought together)
df = df[['pclass', 'sex', 'embarked']].dropna()

# Convert each row to a transaction (basket of categorical attributes)
transactions = df.astype(str).values.tolist()
# Step 1: Create itemsets and count frequency
def get_frequent_itemsets(transactions, min_support=0.1):
    item_counts = defaultdict(int)
    transaction_list = list(map(set, transactions))
    total_tx = len(transaction_list)
    freq_itemsets = []

    # Generate all combinations of size 1 to 3
    for size in range(1, 4):
        for transaction in transaction_list:
            for itemset in combinations(transaction, size):
                item_counts[itemset] += 1

    # Filter based on min support
    for itemset, count in item_counts.items():
        support = count / total_tx
        if support >= min_support:
            freq_itemsets.append((itemset, support))

    return freq_itemsets
# Step 2: Generate association rules from frequent itemsets
def generate_rules(freq_itemsets, min_confidence=0.6):
    rules = []
    itemset_dict = {frozenset(k): v for k, v in freq_itemsets}

    for itemset in itemset_dict:
        if len(itemset) >= 2:
            for i in range(1, len(itemset)):
                for antecedent in combinations(itemset, i):
                    antecedent = frozenset(antecedent)
                    consequent = itemset - antecedent
                    if consequent:
                        confidence = itemset_dict[itemset] / itemset_dict.get(antecedent, 1)
                        if confidence >= min_confidence:
                            rules.append((set(antecedent), set(consequent), confidence))

    return rules
# Run Apriori
frequent_itemsets = get_frequent_itemsets(transactions, min_support=0.1)
rules = generate_rules(frequent_itemsets, min_confidence=0.6)
# Display results
print("Frequent Itemsets:")
for item, support in frequent_itemsets:
    print(f"{set(item)}: support = {support:.2f}")
print("\nAssociation Rules:")
for antecedent, consequent, confidence in rules:
    print(f"{antecedent} => {consequent} (confidence = {confidence:.2f})")